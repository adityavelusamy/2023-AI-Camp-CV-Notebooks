{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n92CF-bSBEIz"
      },
      "source": [
        "<h1>Getting the Environment Set Up</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAQI1JtBH6NX"
      },
      "source": [
        "We will be using tensorflow to create and use our neural networks. Throughout this notebook, we will see just how easy it is to make a working neural network that has a surprisingly high accuary!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ptEQDylSP4SP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up Weights and Biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q wandb\n",
        "import wandb\n",
        "!wandb login --relogin \"YOUR_KEY_HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSQZr0EkBEI3"
      },
      "source": [
        "<h1>Managing our Images</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd-szzHtHCYb",
        "outputId": "e67b479f-d209-41e5-b64b-f7488db5d272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# parameters\n",
        "input_shape = (28, 28) # the image is 28x28\n",
        "num_classes = 10 # 10 labels\n",
        "num_epochs = 5 # 5 training iterations, we bump this down because the model takes longer to train\n",
        "\n",
        "# TODO You can change to the numbers MNIST dataset if you'd like to see how it works on different images\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")\n",
        "\n",
        "# 60,000 images with each image being 28 by 28 pixels\n",
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrnG-YADBEI4"
      },
      "source": [
        "Each image is represented by a 2-D array where each number in the list represents the brightness of the pixel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFHi5C7zBEI4",
        "outputId": "e77117cb-6a71-4108-a456-c8c1bce940f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3zjY4b5BEI4"
      },
      "source": [
        "For our CNN model, we will need this to be a 3-D array for it to work with tensorflow properly. Below we will reshape it to be 3D and then print it out again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rDzIF7VgBEI5"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000,28,28,1))\n",
        "# This just has a massive output, if you want to see the new shape, uncomment it and run this block\n",
        "# train_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l9e_8ttBEI5"
      },
      "source": [
        "We will also need to do this for our test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pZ-U-ZeEBEI5"
      },
      "outputs": [],
      "source": [
        "test_images = test_images.reshape((10000, 28, 28, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lAHMDl8BEI5"
      },
      "source": [
        "That wall of numbers isn't very easy to understand to our brains however. Lets go ahead and display a few of the images from the dataset with matplotlib. If you want to read more about matplot lib, you can go here: https://matplotlib.org/stable/users/index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s2DfOJeRLbwk"
      },
      "outputs": [],
      "source": [
        "def show_imgs():\n",
        "    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = x_train / 255.0\n",
        "    images = x_train[:10]\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 2))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(str(y_train[i]))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "WhBveLKFBEI6",
        "outputId": "1060f1f8-6287-4661-8ac2-f286e992fb3c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x144 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_imgs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foVWkbYzBEI6"
      },
      "source": [
        "<h1>Creating Our Model</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufGBORxkBEI6"
      },
      "source": [
        "Tensorflow lets us create a model super easily. A \"Dense\" layer is just a layer that is fully connected to the next layer, this is the basic type of layer that we are familiar with. Then all we need to do is specify how many neurons there should be in the given layer and tensorflow figures out the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UslclwUyIU6p"
      },
      "outputs": [],
      "source": [
        "# The parameters in each of these Conv2D layers are as follows:\n",
        "# layers.Conv2D(Output_size, Stride, activation, input_shape)\n",
        "# Stride is just how far the square of relevant pixels moves between layers. \n",
        "# Simply put, for this model, the model looks at a 4x4 square of pixels and\n",
        "# then moves 4 pixels to the left before checking the next 4x4 box of pixels\n",
        "def create_model():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(28, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max Pooling just grabs the largest number in a specified area and uses that for the whole area\n",
        "    # below we are looking at a 2x2 square and using the largest number in that square as the value for the whole square\n",
        "\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(56, (3, 3), activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(56, (3, 3), activation='relu'))\n",
        "    \n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dense(10))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83YcVBTMNkE0",
        "outputId": "3ce6dd88-acff-43f5-b686-8d7e37d63059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 28)        280       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 28)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 56)        14168     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 56)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 56)          28280     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 504)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                32320     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75,698\n",
            "Trainable params: 75,698\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "# The optimizer is just an algorithm that helps the AI learn faster\n",
        "# The loss is Sparse Categorical Crossentropy, the name is scary but it just means that \n",
        "# the outputs are converted to percentages and then compared to the expected output\n",
        "# Metrics = accuracy means that we only care about how close the answer was to the actual answer \n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4nlQyOVNmkk"
      },
      "source": [
        "<h1>Training our Model</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx7IdU85BEI7"
      },
      "source": [
        "Tensorflow makes training easy. All we need to do is give it the input data and the expected outputs with the number of epochs to train for and it will handle the rest!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm4C52vBNxBi",
        "outputId": "2eba78ea-6c2c-4313-a237-3f6f628ffafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1621/1875 [========================>.....] - ETA: 9s - loss: 0.5782 - accuracy: 0.8147"
          ]
        }
      ],
      "source": [
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"my-awesome-project\",\n",
        "\n",
        "    # track hyperparameters and run metadata with wandb.config\n",
        "    config={\n",
        "        \"layer_1\": 512,\n",
        "        \"activation_1\": \"relu\",\n",
        "        \"dropout\": random.uniform(0.01, 0.80),\n",
        "        \"layer_2\": 10,\n",
        "        \"activation_2\": \"softmax\",\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"loss\": \"sparse_categorical_crossentropy\",\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"epoch\": 8,\n",
        "        \"batch_size\": 256\n",
        "    }\n",
        ")\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=num_epochs)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VccLuMUBEI7"
      },
      "source": [
        "<h1>Evaluating our Model</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgl6bEj6BEI7"
      },
      "source": [
        "Evaluating out model with tensorflow is super easy. We can see blow that we were able to get about a 90% accuracy on images that our model has never seen before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6XAn4mNr8_W"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f'Accuracy = {test_accuracy} on {test_images.shape[0]} images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbfYrUxSBEI7"
      },
      "source": [
        "But those numbers don't really show the whole story. For example, we do not know which images it is struggling on specifically. To show off our model and give us a chance to see where it is struggling, we will show an image and make a prediction on it. First, we're going to make predictions on the entire test dataset to show off later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJNMGTuBEI7"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgNvbTcBEI7"
      },
      "outputs": [],
      "source": [
        "# pred = model.predict(np.array(test_images[0]))\n",
        "test_images[0].shape\n",
        "np.array(test_images[0]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXG_fSJ8BEI8"
      },
      "source": [
        "Now, we'll generate a random number and use it to show off the corresponding image, prediction and actual label. This allows us to re-run this block over and over to see a range of predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra_kXL0EV2vf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "rand = random.randint(0,9999)\n",
        "\n",
        "plt.subplot()\n",
        "plt.axis('off')\n",
        "plt.imshow(test_images[rand], cmap=plt.cm.binary)\n",
        "plt.show()\n",
        "\n",
        "prediction = np.argmax(predictions[rand])\n",
        "actual = class_names[test_labels[rand]]\n",
        "print(f'Our model predicted: {class_names[prediction]}')\n",
        "print(f'The actual answer was: {actual}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "CV Classification.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
